{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as EM\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, scale\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "def processed_data(name, X, y):\n",
    "    X_raw = X\n",
    "    y_raw =y\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111, stratify=y)\n",
    "\n",
    "    # Preprocessing the data between 0 and 1\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=[0,1]).fit(X)\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    n_targets = len(np.unique(y))\n",
    "    \n",
    "    return {\n",
    "        'name':name,\n",
    "        'X_raw': X_raw,\n",
    "        'y_raw': y_raw,\n",
    "        'X':X,\n",
    "        'y':y,\n",
    "        'X_train':X_train, \n",
    "        'X_test': X_test, \n",
    "        'y_train': y_train, \n",
    "        'y_test': y_test,\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': n_features,\n",
    "        'n_targets': n_targets\n",
    "    }\n",
    "\n",
    "def credit_data():\n",
    "    data=np.genfromtxt('../dataset/german.data-numeric.txt') \n",
    "    X,y = data[:,:-1], data[:,-1:].squeeze() \n",
    "    return processed_data('credit-g',X, y)\n",
    "\n",
    "def banknote_data():\n",
    "    data=np.genfromtxt('../dataset/data_banknote_authentication.txt', delimiter=',') \n",
    "    X,y = data[:,:-1], data[:,-1:].squeeze() \n",
    "    return processed_data('banknote',X,y)\n",
    "\n",
    "def wine_data():\n",
    "    data=np.genfromtxt('../dataset/winequality-white.csv', delimiter=';', skip_header=1)\n",
    "    X,y = data[:,:-1], data[:,-1:].squeeze()\n",
    "    return processed_data('wine-quality',X,y)\n",
    "    \n",
    "# def wine_data():\n",
    "#     data=np.genfromtxt('../dataset/winequality.csv', delimiter=';')\n",
    "#     X,y = data[:,:-1], data[:,-1:].squeeze()\n",
    "#     return processed_data('wine-quality',X,y)\n",
    "\n",
    "def iris_data():\n",
    "    from sklearn.datasets import load_iris\n",
    "    data = load_iris()\n",
    "    X,y = data.data, data.target\n",
    "    return processed_data('iris',X,y)\n",
    "\n",
    "\n",
    "cdata = credit_data()\n",
    "bdata = banknote_data()\n",
    "idata = iris_data()\n",
    "wdata = wine_data()\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "   \n",
    "def bench_k_means(estimator, name, data, labels, sample_size):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    \n",
    "    cluster_labels = estimator.labels_\n",
    "        \n",
    "    return {\n",
    "        'init': name,\n",
    "        'time': (time() - t0),\n",
    "        'accuracy': metrics.accuracy_score(labels, cluster_labels),\n",
    "        'precision': metrics.precision_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'recall': metrics.recall_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'f1_score': metrics.f1_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'homogenity': metrics.homogeneity_score(labels, cluster_labels),\n",
    "        'completeness': metrics.completeness_score(labels, cluster_labels),\n",
    "        'v-measure': metrics.v_measure_score(labels, cluster_labels),\n",
    "        'ARI': metrics.adjusted_rand_score(labels, cluster_labels),\n",
    "        'AMI': metrics.adjusted_mutual_info_score(labels,  cluster_labels),\n",
    "        'mutual_info': metrics.mutual_info_score(labels,  cluster_labels),\n",
    "        'NMI': metrics.cluster.normalized_mutual_info_score(labels, cluster_labels),\n",
    "        'silhouette': metrics.silhouette_score(data, cluster_labels)\n",
    "    }\n",
    "\n",
    "def get_scores(name, data, labels, cluster_labels):\n",
    "    return {\n",
    "        'init': name,\n",
    "        'accuracy': metrics.accuracy_score(labels, cluster_labels),\n",
    "        'precision': metrics.precision_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'recall': metrics.recall_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'f1_score': metrics.f1_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'homogenity': metrics.homogeneity_score(labels, cluster_labels),\n",
    "        'completeness': metrics.completeness_score(labels, cluster_labels),\n",
    "        'v-measure': metrics.v_measure_score(labels, cluster_labels),\n",
    "        'ARI': metrics.adjusted_rand_score(labels, cluster_labels),\n",
    "        'AMI': metrics.adjusted_mutual_info_score(labels,  cluster_labels),\n",
    "        'mutual_info': metrics.mutual_info_score(labels,  cluster_labels),\n",
    "        'NMI': metrics.cluster.normalized_mutual_info_score(labels, cluster_labels),\n",
    "        'silhouette': metrics.silhouette_score(data, cluster_labels)\n",
    "    }\n",
    "\n",
    "def performance_k_means(data, labels, dataset_name, n_clusters_range):\n",
    "    vals = []\n",
    "    for n in n_clusters_range:\n",
    "        estimator = KMeans(init='k-means++', \n",
    "                           n_clusters=n, \n",
    "                           max_iter = 300, \n",
    "                           n_init = 10, \n",
    "                           random_state = 0)\n",
    "        \n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        _time = (time() - t0)\n",
    "        \n",
    "        cluster_labels = estimator.labels_\n",
    "        \n",
    "        _bench = get_scores(\"k-means++\", \n",
    "                                     data, \n",
    "                                     labels, \n",
    "                                     cluster_labels)\n",
    "        _bench['inertia'] = estimator.inertia_\n",
    "        _bench['n_clusters'] = n\n",
    "        _bench['time'] = _time\n",
    "        vals.append(_bench)\n",
    "\n",
    "    df = pd.DataFrame(vals)\n",
    "    return df\n",
    "\n",
    "def performance_em(data, labels, dataset_name, n_clusters_range, covariance_type=\"spherical\"):\n",
    "    vals = []\n",
    "    for n in n_clusters_range:\n",
    "        estimator = EM(n_components=n,\n",
    "                       covariance_type=covariance_type,\n",
    "                       n_init=10,\n",
    "                       warm_start=True,\n",
    "                       random_state=0,\n",
    "                       init_params='kmeans')\n",
    "        \n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        _time = (time() - t0)\n",
    "        \n",
    "        cluster_labels = estimator.predict(data)\n",
    "        \n",
    "        _bench = get_scores(\"gmm-em\", \n",
    "                                     data, \n",
    "                                     labels, \n",
    "                                     cluster_labels)\n",
    "#         _bench['inertia'] = estimator.inertia_\n",
    "        _bench['aic'] = estimator.aic(data)\n",
    "        _bench['bic'] = estimator.bic(data)\n",
    "        _bench['score'] = estimator.score(data)\n",
    "        _bench['n_clusters'] = n\n",
    "        _bench['time'] = _time\n",
    "        vals.append(_bench)\n",
    "\n",
    "    df = pd.DataFrame(vals)\n",
    "    return df\n",
    "\n",
    "\n",
    "def k_means_performance_curve(dataset_name,n_clusters_range, data, labels):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_classes = len(np.unique(labels))\n",
    "    sample_size = n_samples\n",
    "\n",
    "    vals = []\n",
    "    for n in n_clusters_range:\n",
    "        estimator = KMeans(init='k-means++', n_clusters=n, max_iter = 300, n_init = 10, random_state = 0)\n",
    "        _bench = bench_k_means(estimator, name=\"k-means++\", data=data, labels=labels, sample_size=sample_size)\n",
    "        _bench['n_clusters'] = n\n",
    "        vals.append(_bench)\n",
    "\n",
    "    df = pd.DataFrame(vals)    \n",
    "    ax1 = df.plot(x='n_clusters', \n",
    "                  y=['silhouette'], \n",
    "                  title=\"K Means Silhoutte Score %s\"%dataset_name)\n",
    "    ax1.set_xlabel(\"Number of clusters\")\n",
    "    plt.show()\n",
    "        \n",
    "    df = pd.DataFrame(vals)    \n",
    "    ax1 = df.plot(x='n_clusters', \n",
    "                  y=['AMI','silhouette'], \n",
    "                  title=\"K Means Performance evaluation %s\"%dataset_name)\n",
    "    ax1.set_xlabel(\"Number of clusters\")\n",
    "    plt.show()\n",
    "\n",
    "def bench_em(estimator, name, data, labels, sample_size):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    \n",
    "    cluster_labels = estimator.predict(data)\n",
    "        \n",
    "    return {\n",
    "        'init': name,\n",
    "        'time': (time() - t0),\n",
    "        'accuracy': metrics.accuracy_score(labels, cluster_labels),\n",
    "        'precision': metrics.precision_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'recall': metrics.recall_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'f1_score': metrics.f1_score(labels, cluster_labels, average=\"weighted\"),\n",
    "        'homogenity': metrics.homogeneity_score(labels, cluster_labels),\n",
    "        'completeness': metrics.completeness_score(labels, cluster_labels),\n",
    "        'v-measure': metrics.v_measure_score(labels, cluster_labels),\n",
    "        'ARI': metrics.adjusted_rand_score(labels, cluster_labels),\n",
    "        'AMI': metrics.adjusted_mutual_info_score(labels,  cluster_labels),\n",
    "        'mutual_info': metrics.mutual_info_score(labels,  cluster_labels),\n",
    "        'NMI': metrics.cluster.normalized_mutual_info_score(labels, cluster_labels),\n",
    "        'silhouette': metrics.silhouette_score(data, cluster_labels),\n",
    "        'aic': estimator.aic(data),\n",
    "        'bic': estimator.bic(data)\n",
    "    }\n",
    "    \n",
    "def em_performance_curve(dataset_name,n_clusters_range, data, labels,cov_type='spherical'):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_classes = len(np.unique(labels))\n",
    "    sample_size = n_samples\n",
    "\n",
    "    vals = []\n",
    "    for n in n_clusters_range:\n",
    "        estimator = EM(n_components=n,covariance_type=cov_type,n_init=1,warm_start=True,random_state=100,init_params='kmeans')\n",
    "        _bench = bench_em(estimator, name=\"em\", data=data, labels=labels, sample_size=sample_size)\n",
    "        _bench['n_clusters'] = n\n",
    "        vals.append(_bench)\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(vals)    \n",
    "    df.plot(x='n_clusters', ax=ax1,\n",
    "              y=['silhouette'], \n",
    "              title=\"GMM Silhoutte Score %s\"%dataset_name)\n",
    "    ax1.set_xlabel(\"Number of components\")\n",
    "\n",
    "        \n",
    "    df = pd.DataFrame(vals)    \n",
    "    df.plot(x='n_clusters', ax=ax2,\n",
    "                  y=['bic','aic'], \n",
    "                  title=\"BIC and AIC Scores %s\"%dataset_name)\n",
    "    ax2.set_xlabel(\"Number of components\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fig2, (axs) = plt.subplots(1, 1, constrained_layout=True)\n",
    "    \n",
    "    grads = np.gradient(df['bic'].tolist())\n",
    "    axs.plot(df['n_clusters'].tolist(),grads)\n",
    "    axs.set_title(\"Gradient of BIC Scores %s\"%dataset_name)\n",
    "    axs.set_xlabel(\"Number of components\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def silhouette_plot(dataset_name, range_n_clusters, data):\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for %s n_clusters=%s\"%(dataset_name,n_clusters))\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(data[:, 0], data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on %s dataset \"\n",
    "                      \"with n_clusters = %d\" % (dataset_name,n_clusters)),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def pca_scatter_two_component(dataset_name, X, y):\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    v = pca.explained_variance_ratio_\n",
    "    principal_df = pd.DataFrame(data = principal_components, columns = ['pc1','pc2'])\n",
    "    final_df = pd.concat([principal_df, pd.DataFrame({'target': y})],axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    final_df.plot.scatter(x='pc1', y='pc2', c='target', colormap='rainbow', s=10, ax=ax)\n",
    "    ax.set_xlabel(\"pc1 (%0.2f)\"%v[0])\n",
    "    ax.set_ylabel(\"pc2 (%0.2f)\"%v[1])\n",
    "    ax.set_title('2 Component PCA for %s dataset'%dataset_name)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def pca_components(dataset_name, X, y, percentage):\n",
    "    pca = PCA(percentage)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    v = pca.explained_variance_ratio_\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'variance': percentage,\n",
    "        'principle_components': len(v),\n",
    "        'variances': v\n",
    "    }\n",
    "    \n",
    "def pca_performance(data):\n",
    "    ydata_kmeans = []\n",
    "    ydata_em = []\n",
    "    xdata = list(range(1,data['n_features']+1))\n",
    "    \n",
    "    kmeans_orig = KMeans(init='k-means++', n_clusters=data['n_targets'], n_init=10)\n",
    "    kmeans_orig.fit(data['X'])\n",
    "    kmeans_orig_cluster_labels = kmeans_orig.labels_\n",
    "    ydata_kmeans_orig = metrics.silhouette_score(data['X'], kmeans_orig_cluster_labels)\n",
    "    \n",
    "    em_orig = EM(n_components=data['n_targets'],covariance_type=\"spherical\",n_init=1,warm_start=True,random_state=100,init_params='kmeans')\n",
    "    em_orig.fit(data['X'])\n",
    "    em_orig_cluster_labels = em_orig.predict(data['X'])\n",
    "    ydata_em_orig = metrics.silhouette_score(data['X'], em_orig_cluster_labels)\n",
    "    \n",
    "    for i in xdata:\n",
    "        reduced_data = PCA(n_components=i).fit_transform(data['X'])\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=data['n_targets'], n_init=10)\n",
    "        kmeans.fit(reduced_data)\n",
    "        kmeans_cluster_labels = kmeans.labels_\n",
    "        ydata_kmeans.append(metrics.silhouette_score(reduced_data, kmeans_cluster_labels))\n",
    "        \n",
    "        em = EM(n_components=data['n_targets'],covariance_type=\"spherical\",n_init=1,warm_start=True,random_state=100,init_params='kmeans')\n",
    "        em.fit(reduced_data)\n",
    "        em_cluster_labels = em.predict(reduced_data)\n",
    "        ydata_em.append(metrics.silhouette_score(reduced_data, em_cluster_labels))\n",
    "        \n",
    "\n",
    "    #Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "    plt.plot(xdata, ydata_kmeans, label=\"K Means\")\n",
    "    plt.plot(xdata, ydata_em, label=\"EM\")\n",
    "    plt.title('Performance PCA reduced (%s)'%data['name'])\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.ylabel('Silhoutte Score') #within cluster sum of squares\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def two_component_figures(data):\n",
    "    np.random.seed(5)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data['X'])\n",
    "    X = pca.transform(data['X'])\n",
    "    v = pca.explained_variance_ratio_\n",
    "    pl.scatter(X[:,0], X[:, 1], c=data['y'], cmap='rainbow')\n",
    "    pl.xlabel(\"pc1 (%0.2f)\"%v[0])\n",
    "    pl.ylabel(\"pc2 (%0.2f)\"%v[1])\n",
    "    pl.title(\"2 Components PCA %s\"%data['name'])\n",
    "    pl.legend()\n",
    "    pl.show()\n",
    "\n",
    "    ica = ICA(n_components=2)\n",
    "    ica.fit(data['X'])\n",
    "    X = ica.transform(data['X'])\n",
    "    pl.scatter(X[:,0], X[:, 1], c=data['y'], cmap='rainbow')\n",
    "    pl.title(\"2 Components ICA %s\"%data['name'])\n",
    "    pl.show()\n",
    "\n",
    "    rca = RCA(n_components=2)\n",
    "    rca.fit(data['X'])\n",
    "    X = rca.transform(data['X'])\n",
    "    pl.scatter(X[:,0], X[:, 1], c=data['y'], cmap='rainbow')\n",
    "    pl.title(\"2 Components RCA %s\"%data['name'])\n",
    "    pl.show()\n",
    "\n",
    "    lda = LDA(n_components=2)\n",
    "    lda.fit(data['X'], data['y'])\n",
    "    X = lda.transform(data['X'])\n",
    "    pl.scatter(X[:,0],X[:, 1], c=data['y'], cmap='rainbow')\n",
    "    pl.title(\"2 Components LDA %s\"%data['name'])\n",
    "    pl.show()\n",
    "\n",
    "def reduced_data_k_means_perf(data, reduced_data, n_clusters_range, method=\"ICA\", ncluster=None):\n",
    "    dfo = performance_k_means(data['X'], \n",
    "                                  data['y'], \n",
    "                                  data['name'], \n",
    "                                  n_clusters_range=n_clusters_range)\n",
    "\n",
    "    df = performance_k_means(reduced_data, \n",
    "                                  data['y'], \n",
    "                                  data['name'], \n",
    "                                  n_clusters_range=n_clusters_range)\n",
    "\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    print(np.argmax(df['AMI']))\n",
    "    print(np.argmax(df['silhouette']))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax1.plot(dfo['n_clusters'], dfo['AMI'],label=\"AMI\")\n",
    "    ax1.plot(df['n_clusters'], df['AMI'],label=\"AMI \"+method)\n",
    "    ax1.plot(dfo['n_clusters'], dfo['silhouette'],label=\"silhouette\")\n",
    "    ax1.plot(df['n_clusters'], df['silhouette'],label=\"silhouette \"+method)\n",
    "    ax1.set_title(\"K Means Performance evaluation %s\"%data['name'])\n",
    "    ax1.set_xlabel(\"Number of clusters\")\n",
    "    ax1.set_ylabel(\"Scores\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(dfo['n_clusters'], dfo['inertia'],label=\"inertia\")\n",
    "    ax2.plot(df['n_clusters'], df['inertia'],label=\"inertia \"+method)\n",
    "    ax2.set_title(\"K Means Inertia %s\"%data['name'])\n",
    "    ax2.set_xlabel(\"Number of clusters\")\n",
    "    ax2.set_ylabel(\"Scores\")\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "    fig, ax3 = plt.subplots(1,1)\n",
    "    ax3.plot(df['n_clusters'], df['time'], label=\"time\")\n",
    "    ax3.plot(dfo['n_clusters'], dfo['time'], label=\"time \"+method)\n",
    "    ax3.set_title(\"K Means Time %s\"%data['name'])\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if not ncluster:\n",
    "        ncluster = data['n_targets']\n",
    "        \n",
    "    perfo = dfo[df['n_clusters']==ncluster]\n",
    "    perf = df[df['n_clusters']==ncluster]\n",
    "\n",
    "    print(perfo)\n",
    "    print(\"------\")\n",
    "    print(perf)\n",
    "    \n",
    "\n",
    "def reduced_data_em_perf(data, reduced_data, n_clusters_range, method=\"ICA\", ncluster=None):\n",
    "    dfo = performance_em(data['X'], \n",
    "                                  data['y'], \n",
    "                                  data['name'], \n",
    "                                  n_clusters_range=n_clusters_range)\n",
    "\n",
    "    df = performance_em(reduced_data, \n",
    "                          data['y'], \n",
    "                          data['name'], \n",
    "                          n_clusters_range=n_clusters_range)\n",
    "\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    print(np.argmax(df['AMI']))\n",
    "    print(np.argmax(df['silhouette']))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax1.plot(dfo['n_clusters'], dfo['AMI'],label=\"AMI\")\n",
    "    ax1.plot(df['n_clusters'], df['AMI'],label=\"AMI \"+method)\n",
    "    ax1.plot(dfo['n_clusters'], dfo['silhouette'],label=\"silhouette\")\n",
    "    ax1.plot(df['n_clusters'], df['silhouette'],label=\"silhouette \"+method)\n",
    "    ax1.set_title(\"EM Performance evaluation %s\"%data['name'])\n",
    "    ax1.set_xlabel(\"Number of clusters\")\n",
    "    ax1.set_ylabel(\"Scores\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(dfo['n_clusters'], dfo['bic'],label=\"bic\")\n",
    "    ax2.plot(df['n_clusters'], df['bic'],label=\"bic \"+method)\n",
    "    ax2.set_title(\"EM BIC %s\"%data['name'])\n",
    "    ax2.set_xlabel(\"Number of clusters\")\n",
    "    ax2.set_ylabel(\"Scores\")\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "    fig, ax3 = plt.subplots(1,1)\n",
    "    ax3.plot(df['n_clusters'], df['time'], label=\"time\")\n",
    "    ax3.plot(dfo['n_clusters'], dfo['time'], label=\"time \"+method)\n",
    "    ax3.set_title(\"EM Time %s\"%data['name'])\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if not ncluster:\n",
    "        ncluster = data['n_targets']\n",
    "        \n",
    "    perfo = dfo[df['n_clusters']==ncluster]\n",
    "    perf = df[df['n_clusters']==ncluster]\n",
    "\n",
    "    print(perfo)\n",
    "    print(\"------\")\n",
    "    print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
